# ============================================================================
# Evaluation configuration file. Pass its path to the program (option '-c') to
# produce the desired evaluation in form of a HDF file whose structure (similar
# to this config structure), is described at the bottom of this page
# ============================================================================


# Model: classifier algorithm and parameters + training set
model:
  # The Python path of the scikit-learn classifier:
  classifier: "sklearn.ensemble.IsolationForest"
  # The model parameters to iterate over (see the classifier `__init__` method):
  parameters:
    n_estimators:
      - 25
      - 50
    max_samples:
      - 512
      - 1024
      - 2048
    contamination:
      - 'auto'
    random_state:
      - 11

  # training set:
  training_set:
    # Path to the training set file(s) in HDF table format, with instances arranged
    # by row and features arranged by columns. For info see:
    # https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables.
    # Non absolute paths will be relative to this file directory. If the HDF has
    # several tables, the group identifier can be specified as suffix after a double
    # colon "::", e.g. "myfile.hdf::table1"; for info see:
    # https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html
    file_path:
      - 'trainingset.hdf'

    # The training set(s) column name denoting the true labels or true numeric
    # values (argument 'y' of the classifier `fit` method). Provide the empty
    # string, null or nothing if the classifier can be fitted without labels
    # (e.g. `sklearn.ensemble.IsolationForest`)
    ground_truth_column: null

    # The training sets column name denoting the sample weights to be used when fitting
    # the classifier (method `fit`). Provide the empty string, null or nothing to
    # ignore sample weights
    sample_weight_column: null

    # Features to use. I.e. columns of all datasets (training and validation sets).
    # When providing features as list of strings, the evaluation will be performed
    # on all combinations automatically. So for example:
    # features:
    #   - PGA
    #   - PGV
    # will run evaluations for 3 classifiers: first using PGA alone, then PGV, and
    # finally both PGA and PGV. To avoid this, and use a specific combination of
    # your choice, provide a list of lists. For instance, the same configuration as
    # above can be obtained by typing explicitly (note the list of lists):
    # features:
    # -
    #  - PGA
    # -
    #  - PGV
    # -
    #  - PGA
    #  - PGV
    features:
      - PGA
      - PGV

    # Drop / ignore missing values (aka not available, or NA) from each computation.
    # NA values are NaN and None. Set to true if the classifier can not handle NA
    drop_na: true

    # Whether to consider also +-Inf as NA (ignored if `drop_na` is false)
    inf_is_na: true


# Evaluation: assess the model(s) performances by means of several evaluation
# metrics describing how close are true values and model-predicted values on a
# separate validation set, or on the same training set and cross validation
evaluation:

  # Path to the validation set file(s), or to a scikit-learn splitter class
  # (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)
  # that will be used to cross-validate the training set(s).
  # For files, the same rules apply as in the training set `file_path` (see above).
  # For splitter classes, provide the scikit-learn class name with optional
  # arguments, e.g.: "StratifiedKFold()", "KFold(n_splits=5)" or the full path,
  # e.g.: "sklearn.model_selection.KFold(5)". You can also provide both files and
  # splitter classes if you want, e.g. to perform in a single run cross
  # validation and use the validation files as test set
  validation:
    - 'validationset_bad.hdf'
    - 'validationset_good.hdf'

  # The validation set(s) column name denoting the true labels or true numeric
  # values (argument `y_true` of scikit-learn metrics functions). In case of
  # cross-validation (see `evaluation.validation`) it must be a column of the
  # training set, and it is usually the same as the ground truth column used for
  # fitting the model, if given
  ground_truth_column: 'outlier'

  # The validation set(s) column name denoting the sample weights to be used when
  # evaluating the model (argument `sample_weight` of scikit-learn metrics
  # functions). In case of cross-validation (see `evaluation.validation`) it must
  # be a column of the training set, and doesn't need to be the same as the sample
  # weight column used for fitting the model, if given. Provide the empty string,
  # null or nothing to ignore sample weights
  sample_weight_column: null

  # The function used by each trained model to classify or predict new instances.
  # Usually, it is the full path or simply the name of a classifier method (e.g.
  # "decision_function", "sklearn.ensemble.IsolationForest.predict". For details
  # see https://scikit-learn.org/stable/glossary.html#methods), but it can also
  # be the path to any user-defined function with signature `(clf, X)` where `clf`
  # is the classifier object and `X` is the data matrix to be classified or predicted
  # In any case, be sure that the output of this function is consistent with the
  # values provided in the ground truth column.
  # E.g., if ground truth labels are either True or False, a prediction function
  # can return continuous scores in [0, 1], or binary labels in {0, 1}, but not
  # scores in [0, +Inf] or labels in {-1, 1}
  prediction_function: 'evalutilities.isf_anomaly_score'  # <- see associated Python file

  # Evaluation metrics, as list of Python function paths. You can type standard
  # metrics (https://scikit-learn.org/stable/modules/model_evaluation.html) or
  # user-defined ones with signature `(y_true, y_pred, *, sample_weight=None)`,
  # where `y_true` is the array of ground truth values (see `ground_truth_column`),
  # `y_pred` the predicted values or labels (see `prediction_function`), and
  # sample_weight is the optional array of weights (see `sample_weight_column`).
  # All functions must return either a numeric scalar or a dict of metric names
  # (`str`) mapped to their numeric value (in the former case, the function name
  # will be used as metric name)
  metrics:
    - sklearn.metrics.average_precision_score
    - evalutilities.best_th_prc  # <- see associated Python file


# Evaluation output file description

# The evaluation result file produced by the program by means of the
# configuration file is an HDF file composed by 2 linked HDF tables. In Python,
# you can open the tables via the `pandas` library:
#
# ```python
# import pandas as pd
# models_dataframe = pd.read_hdf(eval_result_file_path, key="models")
# evaluations_dataframe = pd.read_hdf(eval_result_file_path, key="evaluations")
# ```
#
# **Table "models"**
#
# Each row denotes a model created with a specific combination of the values
# input in the config file. Each column denote:
#
# | Column        | Type | Description                                        |
# |---------------|------|----------------------------------------------------|
# | id            | int  | Unique model id                                    |
# | clf           | str  | Python path of the scikit-learn classifier used    |
# | param_`name1` | any  | classifier parameters, prefixed with "param_"      |
# | param_`name2` | any  | (see above)                                        |
# | ...           | ...  | ...                                                |
# | training_set  | str  | The training-set path                              |
# | feat_`name1`  | bool | The training-set features used by the model, prefixed with "feat_" (true means: feature used) |
# | feat_`name2`  | bool | (see above)                                        |
# | ...           | ...  | ...                                                |
# | ground_truth_column | str  | The training set column denoting the true labels/numeric values used to fit the model (can be null if the classifier `fit` method needs no labels) |
# | sample_weight_column | str  | The training set column denoting the optional sample weights to fit the model (empty: no weights) |
# | drop_na       | bool | Remove values not avaliable (NA) from the training set. True if the classifier cannot handle NA values (by default NaN and None) |
# | inf_is_na     | bool | Whether +-Inf should also be considered NA                    |
#
#
# **Table "evaluations"**
#
# Each row denotes a model evaluation created with a specific combination of
# the values input in this config file. Each column denote:
#
# | Column               | Type | Description                                 |
# |----------------------|------|---------------------------------------------|
# | model_id             | int  | The unique id of the model evaluated (see table above)       |
# | validation_set       | str  | The validation set path. If this equals the model's training set path, then a cross validation was performed, splitting the training set by means of the object specified in the column `cv_splitter` |
# | cv_splitter          | bool | In case of cross validation, it is the path of the scikit-learn splitter used. When null (check it with `pd.isna`), then the `validation_set` is provided as file and most likely distinct from the training set |
# | ground_truth_column  | str  | The validation set column denoting the true labels (passed as argument `y_true` in `sklearn.metrics` functions) |
# | sample_weight_column | str  | The validation set column denoting the optional sample weights (empty: no weights) |
# | prediction_function  | str  | The function or classifier method used for prediction (passed as agument `y_pred`/`y_score` in `sklearn.metrics` functions) |
# | metric_`name1`       | any  | The evaluation metrics, prefixed with "metric_")     |
# | metric_`name2`       | any  | (see above)                                          |
# | ...                  | ...  | ...                                                  |
#