# ============================================================================
# Evaluation configuration file. Edit / rename at your wish and pass its path
# to the program (option '-c') to produce the desired evaluation result whose
# structure is described at the bottom of this page
# ============================================================================

# Features to use, which must be columns of the tables loaded from any given
# training and test set.
# When providing a list of names/strings, the evaluation will be performed on
# all features combinations automatically. So for example:
# features:
#   - PGA
#   - PGV
# will run evaluations for 3 classifiers: first using PGA alone, then PGV, and
# finally both PGA and PGV. To avoid this, and use a specific combination of
# your choice, provide a list of lists. For instance, the same configuration as
# above can be obtained by typing explicitly (note the list of lists):
# features:
# -
#  - PGA
# -
#  - PGV
# -
#  - PGA
#  - PGV
features:
  - PGA
  - PGV

# Path to the training set file(s) in HDF table format. Non absolute paths will be
# relative to this file directory. If the HDF has several tables, arranged under
# different directory-like structures called groups, you can specify the group
# name ('key' argument of `pandas.read_hdf`) as suffix after a double colon "::".
# Each training set table must have instances arranged by row, and all necessary
# features arranged by column
training_set:
  - 'trainingset.hdf'

# Classifier:
classifier:
  # The Python path of the sklearn classifier:
  classname: "sklearn.ensemble.IsolationForest"
  # The classifier parameters to iterate over (you can type any parameters used
  # to initialize the classifier):
  parameters:
    n_estimators:
      - 25
      - 50
      - 100
    max_samples:
      - 512
      - 1024
      - 2048
    contamination:
      - 'auto'
    random_state:
      - 11
      - 25
      - 42

# Drop / ignore missing values (aka not available, or NA) from each computation.
# NA values are NaN and None. Set to true if the classifier can not handle NA
drop_na: true

# Whether to consider also +-Inf as NA (ignored if `drop_na` is false)
inf_is_na: true

# Path to the test set file(s) in HDF table format. Non absolute paths will be
# relative to this file directory. If the HDF has several tables, arranged under
# different directory-like structures called groups, you can specify the group
# name ('key' argument of `pandas.read_hdf`) as suffix after a double colon "::".
# Each test set table must have instances arranged by row, all necessary
# features arranged by column, plus an additional column denoting the instances
# ground truth value (see `ground_truth_column`) and, optionally, a column called
# "sample_weight" with instances weight (see `evaluation_metrics`)
validation_set:
  - 'validationset_bad.hdf'
  - 'validationset_good.hdf'


# The column name denoting the true labels or numeric values (argument `y_true`
# in most `sklearn.metrics` functions). It must be a column present in all test
# set tables, and will be used by the evaluation metrics to compare true
# values with those predicted by the classifier (see `prediction_function`)
ground_truth_column: 'outlier'


# The function used by the classifier to classify or predict test set instances.
# Usually, it is the full path or simply the name of a classifier method (e.g.
# 'decision_function', 'predict', 'sklearn.ensemble.IsolationForest.predict'),
# but it can also be the path to any user-defined function with signature
# `(clf, X)` where `clf` is the classifier object and `X` is the data matrix
# to be classified or predicted. In any case, be sure that the output of this
# function is consistent with the values provided as ground truth in the test
# set(s) (see `ground_truth_column`). E.g., if ground truth labels are either
# True or False, a prediction function can return continuous scores in [0, 1],
# or binary labels in {0, 1}, but not scores in [0, +Inf] or labels in {-1, 1}
prediction_function: 'evalutilities.isf_anomaly_score'  # <- see associated Python file


# Evaluation metrics, as list of Python function paths (for a list of common
# metrics, see https://scikit-learn.org/stable/modules/model_evaluation.html)
# For user-defined metrics, each metric function must have signature
# `(y_true, y_pred, *, sample_weight=None)`, where `y_true` is the array of
# ground truth values (see `ground_truth_column`), `y_pred` the predicted
# values or labels (see `prediction_function`), and sample_weight is the array
# of weights that will be taken from the column named "sample_weight" of each
# test set to evaluate, if such a column exist (otherwise, ignore weights).
# Each function can return either a numeric scalar or a dict of metric names
# (`str`) mapped to their numeric value. In the former case, the metric name
# will be set as the function name.
evaluation_metrics:
  - sklearn.metrics.average_precision_score
  - evalutilities.best_th_prc  # <- see associated Python file

# Evaluation output file description
#
# The evaluation result file produced by the program by means of the
# configuration file is an HDF file composed by 2 linked HDF tables. In Python,
# you can open the tables via the `pandas` library:
#
# ```python
# import pandas as pd
# models_dataframe = pd.read_hdf(eval_result_file_path, key="models")
# evaluations_dataframe = pd.read_hdf(eval_result_file_path, key="evaluations")
# ```
#
# **Table "models"**
#
# Each row denotes a model created with a specific combination of the values
# input in the config file. Each column denote:
#
# | Column        | Type | Description                                        |
# |---------------|------|----------------------------------------------------|
# | id            | int  | unique model id                                    |
# | clf           | str  | the classifier name                                |
# | param_`name1` | any  | the model parameters, prefixed with "param_"       |
# | ...           | ...  |                                                    |
# | param_`nameN` | any  |                                                    |
# | training_set  | str  | The training-set path         |
# | feat_`name1`  | bool | The training-set features used by the model, prefixed with "feat_" (true means: feature used) |
# | ...           | ...  |                                                    |
# | feat_`nameN`  | bool |                                                    |
# | drop_na       | bool | If the classifier algorithm can not handle missing values (by default NaN and None) |
# | inf_is_na     | bool | Whether +-Inf are considered NA                    |
#
#
# **Table "evaluations"**
#
# Each row denotes a model evaluation created with a specific combination of
# the values input in this config file. Each column denote:
#
# | Column               | Type | Description                                 |
# |----------------------|------|---------------------------------------------|
# | model_id             | int  | The unique model id (see table above)       |
# | validation_set       | str  | The validation-set path                     |
# | ground_truth_column  | str  | The validation set column denoting the true labels (passed as argument `y_true` in `sklearn.metrics` functions) |
# | prediction_function  | str  | the function or classifier method used for prediction (passed as agument `y_pred`/`y_score` in `sklearn.metrics` functions) |
# | evalmetric_`name1`   | any  | The evaluation metrics, prefixed with "evalmetric_") |
# | ...                  | ...  |                                             |
# | evalmetric_`nameN`   | any  |                                             |
